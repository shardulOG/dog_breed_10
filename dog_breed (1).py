# -*- coding: utf-8 -*-
"""dog_breed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5y91MZBuJzi3lwYlP4bb_Yk6ICPCjKu
"""

import tensorflow as tf
import tensorflow_hub as hub
print("Tensorflow version:",tf.__version__)
print("Tensorflow_hub version:",hub.__version__)

import pandas as pd
label_csv=pd.read_csv("/content/drive/MyDrive/dog_breed_classification/labels.csv")

import numpy as np
labels= label_csv["breed"].to_numpy() # convert dfs column to NumPy array
labels[:10]

# Find the unique df values
unique_breeds = np.unique(labels)
len(unique_breeds)

given_breeds=["beagle","chihuahua","doberman","french_bulldog","golden_retriever","malamute","pug","saint_bernard","scottish_deerhound","tibetan_mastiff"]

unique_breeds_list= unique_breeds.tolist()
unique_breeds_list

delete_list= [breeds for breeds in  unique_breeds_list if breeds not in given_breeds]

delete_list

label_csv.head()

new_df_indexed=label_csv.set_index("breed")

new_df_indexed.head()

final_data=new_df_indexed.drop(delete_list)

df=final_data.reset_index()

df.shape

label_csv.shape

df["breed"].value_counts().plot(kind="bar",figsize=(20,8));

from IPython.display import display, Image
# Image("drive/My Drive/Data/train/000bec180eb18c7604dcecc8fe0dba07.jpg")

filenames=["/content/drive/MyDrive/dog_breed_classification/train/"+fname+".jpg" for fname in df["id"]]
filenames[:10]

import numpy as np
if len(df)==len(filenames):
  print("match")
else:
  print("no")

import numpy as np
dfs=df["breed"]
dfs=np.array(dfs)
len(dfs)

unique_breeds_=np.unique(dfs)
unique_breeds_

dfs[0]==unique_breeds_

# Turn every df into a boolean array
boolean_dfs = [df== np.array(unique_breeds_) for df in dfs]
boolean_dfs[:3]

# Example: Turning a boolean array into integers
print(dfs[0]) # original df
print(np.where(unique_breeds_== dfs[0])[0][0]) # index where df occurs
print(boolean_dfs[0].argmax()) # index where df occurs in boolean array
print(boolean_dfs[0].astype(int)) # there will be a 1 where the sample df occurs

len(boolean_dfs)

# Setup X & y variables
X = filenames
y = boolean_dfs

len(X)

len(y)

NUM_IMAGES= 800 #@param {type:"slider",min:100,max:841,step:100}

from sklearn.model_selection import train_test_split
X_train,X_val,y_train,y_val=train_test_split(X[:NUM_IMAGES],
                                             y[:NUM_IMAGES],
                                             test_size=0.2,
                                             random_state=42)

len(X_train), len(y_train), len(X_val), len(y_val)

X_train[:5],y_train[:2]

# Convert image to NumPy array
from matplotlib.pyplot import imread
image = imread(filenames[42]) # read in an image
image.shape

# Define image size
IMG_SIZE = 224

def process_image(image_path):
  """
  Takes an image file path and turns it into a Tensor.
  """
  # Read in image file
  image = tf.io.read_file(image_path)
  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)
  image = tf.image.decode_jpeg(image, channels=3)
  # Convert the colour channel values from 0-225 values to 0-1 values
  image = tf.image.convert_image_dtype(image, tf.float32)
  # Resize the image to our desired size (224, 244)
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
  return image

# Create a simple function to return a tuple (image, df)
def get_image_df(image_path, df):
  """
  Takes an image file path name and the associated df,
  processes the image and returns a tuple of (image, df).
  """
  image = process_image(image_path)
  return image, df

# Define the batch size, 32 is a good default
BATCH_SIZE = 32

# Create a function to turn data into batches
def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  """
  Creates batches of data out of image (x) and df (y) pairs.
  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.
  Also accepts test data as input (no dfs).
  """
  # If the data is a test dataset, we probably don't have dfs
  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x))) # only filepaths
    data_batch = data.map(process_image).batch(BATCH_SIZE)
    return data_batch
  
  # If the data if a valid dataset, we don't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths
                                               tf.constant(y))) # dfs
    data_batch = data.map(get_image_df).batch(BATCH_SIZE)
    return data_batch

  else:
    # If the data is a training dataset, we shuffle it
    print("Creating training data batches...")
    # Turn filepaths and dfs into Tensors
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths
                                              tf.constant(y))) # dfs
    
    # Shuffling pathnames and dfs before mapping image processor function is faster than shuffling images
    data = data.shuffle(buffer_size=len(x))

    # Create (image, df) tuples (this also turns the image path into a preprocessed image)
    data = data.map(get_image_df)

    # Turn the data into batches
    data_batch = data.batch(BATCH_SIZE)
  return data_batch

# Create training and validation data batches
train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val, y_val, valid_data=True)

# Check out the different attributes of our data batches
train_data.element_spec, val_data.element_spec

import matplotlib.pyplot as plt

# Create a function for viewing images in a data batch
def show_25_images(images, dfs):
  """
  Displays 25 images from a data batch.
  """
  # Setup the figure
  plt.figure(figsize=(10, 10))
  # Loop through 25 (for displaying 25 images)
  for i in range(25):
    # Create subplots (5 rows, 5 columns)
    ax = plt.subplot(5, 5, i+1)
    # Display an image
    plt.imshow(images[i])
    # Add the image df as the title
    plt.title(unique_breeds_[dfs[i].argmax()])
    # Turn gird lines off
    plt.axis("off")

# Visualize training images from the training data batch
train_images, train_dfs = next(train_data.as_numpy_iterator())
show_25_images(train_images, train_dfs)

# Visualize validation images from the validation data batch
val_images, val_dfs = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_dfs)

# Setup input shape to the model
INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels

# Setup output shape of the model
OUTPUT_SHAPE = len(unique_breeds_) # number of unique dfs

# Setup model URL from TensorFlow Hub
MODEL_URL = "https://tfhub.dev/tensorflow/resnet_50/classification/1"

# Create a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", MODEL_URL)

  # Setup the model layers
  model = tf.keras.Sequential([
    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)
    tf.keras.layers.Dense(units=OUTPUT_SHAPE, 
                          activation="softmax") # Layer 2 (output layer)
  ])

  # Compile the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(), # Our model wants to reduce this (how wrong its guesses are)
      optimizer=tf.keras.optimizers.Adam(), # A friend telling our model how to improve its guesses
      metrics=["accuracy",] # We'd like this to go up
  )

  # Build the model
  model.build(INPUT_SHAPE) # Let the model know what kind of inputs it'll be getting
  
  return model

# Create a model and check its details
model= create_model()
model.summary()

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

import datetime
import os
# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log directory for storing TensorBoard logs
  logdir = os.path.join("drive/MyDrive/dog_breed_classification/logs",
                        # Make it so the logs get tracked whenever we run an experiment
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

# Create early stopping (once our model stops improving, stop training)
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                  patience=3) # stops after 3 rounds of no improvements

# How many rounds should we get the model to look through the data?
NUM_EPOCHS = 100 #@param {type:"slider", min:10, max:100, step:10}

# Build a function to train and return a trained model
def train_model():
  """
  Trains a given model and returns the trained version.
  """
  # Create a model
  model = create_model()

  # Create new TensorBoard session everytime we train a model
  tensorboard = create_tensorboard_callback()

  # Fit the model to the data passing it the callbacks we created
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1, # check validation metrics every epoch
            callbacks=[tensorboard, early_stopping])
  
  return model

# Fit the model to the data
model = train_model()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/dog_breed_classification/logs

# Make predictions on the validation data (not used to train on)
predictions = model.predict(val_data, verbose=1) # verbose shows us how long there is to go
predictions

# Check the shape of predictions
predictions.shape

# First prediction
print(predictions[0])
print(f"Max value (probability of prediction): {np.max(predictions[0])}") # the max probability value predicted by the model
print(f"Sum: {np.sum(predictions[0])}") # because we used softmax activation in our model, this will be close to 1
print(f"Max index: {np.argmax(predictions[0])}") # the index of where the max value in predictions[0] occurs
print(f"Predicted label: {unique_breeds_[np.argmax(predictions[0])]}") # the predicted label

# Turn prediction probabilities into their respective label (easier to understand)
def get_pred_label(prediction_probabilities):
  """
  Turns an array of prediction probabilities into a label.
  """
  return unique_breeds_[np.argmax(prediction_probabilities)]

# Get a predicted label based on an array of prediction probabilities
pred_label = get_pred_label(predictions[0])
pred_label

# Check again if GPU is available (otherwise computing will take a looooonnnnggggg time)
print("GPU", "available (YESS!!!!)" if tf.config.list_physical_devices("GPU") else "not available :(")

# Create a function to unbatch a batched dataset
def unbatchify(data):
  """
  Takes a batched dataset of (image, label) Tensors and returns separate arrays
  of images and labels.
  """
  images = []
  dfs = []
  # Loop through unbatched data
  for image,label in data.unbatch().as_numpy_iterator():
    images.append(image)
    dfs.append(unique_breeds_[np.argmax(label)])
  return images, dfs

# Unbatchify the validation data
val_images, val_labels = unbatchify(val_data)
val_images[0], val_labels[0]

def plot_pred(prediction_probabilities, dfs, images, n=1):
  """
  View the prediction, ground truth label and image for sample n.
  """
  pred_prob, true_label, image = prediction_probabilities[n], dfs[n], images[n]
  
  # Get the pred label
  pred_label = get_pred_label(pred_prob)
  
  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the color of the title depending on if the prediction is right or wrong
  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  plt.title("{} {:2.0f}% ({})".format(pred_label,
                                      np.max(pred_prob)*100,
                                      true_label),
                                      color=color)

# View an example prediction, original image and truth label
plot_pred(prediction_probabilities=predictions,
          dfs=val_labels,
          images=val_images)

def plot_pred_conf(prediction_probabilities, dfs, n=1):
  """
  Plots the top 10 highest prediction confidences along with
  the truth label for sample n.
  """
  pred_prob, true_label = prediction_probabilities[n], dfs[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  # Find the top 10 prediction labels
  top_10_pred_labels = unique_breeds_[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)), 
                     top_10_pred_values, 
                     color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")

  # Change color of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               dfs=val_labels,
               n=9)

# Let's check a few predictions and their different values
i_multiplier = 0
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(5*2*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions,
            dfs=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                dfs=val_labels,
                n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

def save_model(model, suffix=None):
  """
  Saves a given model in a models directory and appends a suffix (str)
  for clarity and reuse.
  """
  # Create model directory with current time
  modeldir = os.path.join("/content/drive/MyDrive/dog_breed_classification/models",
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5" # save format of model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

def load_model(model_path):
  """
  Loads a saved model from a specified path.
  """
  print(f"Loading saved model from: {model_path}")
  model = tf.keras.models.load_model(model_path,
                                     custom_objects={"KerasLayer":hub.KerasLayer})
  return model

# Save our model trained on 1000 images
save_model(model, suffix="800-images-Adam")

# Load our model trained on 1000 images
model_800_images = load_model('/content/drive/MyDrive/dog_breed_classification/models/20210617-18111623953468-800-images-Adam.h5')

# Evaluate the pre-saved model
model.evaluate(val_data)

# Remind ourselves of the size of the full dataset
len(X), len(y)

# Turn full training data in a data batch
full_data = create_data_batches(X, y)

# Instantiate a new model for training on the full dataset
full_model = create_model()

# Create full model callbacks

# TensorBoard callback
full_model_tensorboard = create_tensorboard_callback()

# Early stopping callback
# Note: No validation set when training on all the data, therefore can't monitor validation accruacy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy",
                                                             patience=3)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/dog_breed_classification/logs

# Fit the full model to the full training data
full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_tensorboard, 
                          full_model_early_stopping])

# Save model to file
save_model(full_model, suffix="all-images-Adam")

# Load in the full model
loaded_full_model = load_model('/content/drive/MyDrive/dog_breed_classification/models/20210617-18591623956375-all-images-Adam.h5')

# Get custom image filepaths
custom_path = "/content/drive/MyDrive/dog_breed_classification/my_dogs_photos/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path)]

custom_image_paths

# Turn custom image into batch (set to test data because there are no labels)
custom_data = create_data_batches(custom_image_paths, test_data=True)
custom_data

# Make predictions on the custom data
custom_preds = loaded_full_model.predict(custom_data)

# Get custom image prediction labels
custom_pred_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_labels

# Get custom images (our unbatchify() function won't work since there aren't labels)
custom_images = []
# Loop through unbatched data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

# Check custom image predictions
plt.figure(figsize=(10, 10))
for i, image in enumerate(custom_images):
  plt.subplot(1, 6, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_labels[i])
  plt.imshow(image)

!pip install fastapi

!pip install colabcode

!pip install streamlit

!pip install -U ipykernel

!pip install  -q streamlit

!pip install pyngrok

!ngrok authtoken 1u5nXgu133MRfirrM9t4BhB6JUZ_wTpXBoFhq3Fp4XYZ74wg

# Commented out IPython magic to ensure Python compatibility.
# %%writefile score.py
# import tensorflow as tf
# import streamlit as st
# import numpy as np
# from PIL import Image
# import requests
# from io import BytesIO
# 
# st.set_option('deprecation.showfileUploaderEncoding',False)
# st.title("Dog_breed_classification")
# 
# @st.cache(allow_output_mutation=True)
# def load_model():
#   model=tf.keras.models.load_model("/content/drive/MyDrive/dog_breed_classification/models/20210618-09331624008814-all-images-Adam.h5")
#   return model
# 
# 
# with st.spinner("Loading model into memory..."):
#   model=load.model()
# 
# classes=["beagle","chihuahua","doberman","french_bulldog","golden_retriever","malamute","pug","saint_bernard","scottish_deerhound","tibetan_mastiff"]
# 
# 
# def scale(image):
#   image=tf.cast(image,tf.float32)
#   image /=255
#   return tf.image.resize(image,[244,244])
# 
# def decode_img(image):
#   img=tf.image.decode_jpeg(image,channels=3)
#   img=scale(img)
#   return np.expand_dims(img,axis=0)
# 
# 
# path=st.text_input("Enter image URL to classify...","https://www.petpaw.com.au/wp-content/uploads/2014/04/Doberman-Pinscher-1.jpg")
# if path is not None:
#   content=requests.get(path).content
# 
#   st.write("Predicted class:")
#   with st.spinner("Classifying..."):
#     label=np.argmax(model.predict(decode_img(content)),axis=1)
#     st.write(classes[label(0)])
#   st.write("")
#   image=Image.open(BytesIO(content))
#   st.image(image,caption="classifying dog breeds",use_column_width=True)

!nohup streamlit run score.py &

from pyngrok import ngrok
url=ngrok.connect(port=8501)
url

!cat nohup.out

